{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WX + B"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In all machine learning algorithm right weights are to be found\n",
    "\n",
    "W.T :- for dot product , it is transposed\n",
    "\n",
    "rows of first must be equal to columns of the second matrix"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "WHOLE PROCESS OF NEURAL NETWORK\n",
    "\n",
    "FORWARD PROPAGATION STEPS\n",
    "1. Genenerate w with transpose size of X matrix\n",
    "2. dot_product(X, W.T)\n",
    "3. after dot product add bais \"b\"\n",
    "4. at the end apply an activtaion functionon each element \n",
    "\n",
    "\n",
    "BACKWARD PROPAGATION\n",
    "5. Finally predict value of y_predict\n",
    "6. Calculate loss (y-y_predict)\n",
    "7. Updare W and B values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# relu(dot(W, INPUT) + B)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "relu returns value > zero \n",
    "values < 0 or 0 are considered 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADIENT BASED OPTIMIZATION\n",
    "\n",
    "Gradient means random, derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "when non linear line(curve) , and we have to tell where the next point lies after\n",
    "standing at one point we take derivative\n",
    "it will provide the value of Y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "derivatiev computes the slope\n",
    "slope of the error \n",
    "loss generated\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from the slope , global minima is searched\n",
    "the point where the loss is minimum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# output = relu(dot(W, input)+ b)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "weights / trainable parameters\n",
    "their adjust or finding best suitable value of\n",
    "w and b is called training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values of w are randomly initialized \n",
    "so termed as RANDOM INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradullay optimizer updates the values so the\n",
    "results goes towards th best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative is taken of loss, loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING LOOP"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Draw a batch of training samples X and corresponding targets Y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2.Run teh network on X :- FORWARD PASS \n",
    "to obtain y_predict                     \n",
    "..........tensor operation on"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3.compute the loss of function : \n",
    "    a ,measure of the mismatch between y_pred and y         \n",
    "    ..... tensor function"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4.update all weights to lower the loss"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "optimizer slope nikalta aa\n",
    "if + then it adds negative value \n",
    "if _ then it adds positive value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR LINEAR EQUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(X):\n",
    "    return x\n",
    "x=20\n",
    "y=f(x)\n",
    "\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,epsilon_x):\n",
    "    return x +  epsilon_x\n",
    "x = 20+0.5\n",
    "y = f(20,0.5)\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bais/ intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CURVE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f( x + epsilon_x) = y + a* epsilon_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jo b slope/derivative aata aa osky oposite\n",
    "direction mein w and b ki values update hongy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def abc(x):\n",
    "    return x + 2\n",
    "\n",
    "abc(abc(abc(2)))\n",
    "abc(abc(4))\n",
    "abc(6)\n",
    "8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOCHASTIC GRADIENT DESCENT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stochastic means random\n",
    "It gives reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnable, trainable parametrs :- w and b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step size : jump \n",
    "\n",
    "    \"learning rate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large step size may result in not reaching at the desired point\n",
    "required accuracy may not be acquired\n",
    "calculation fast\n",
    "not nearest to 0 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small step size\n",
    "loss nearest to zero but time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss of derivative is in n dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w , loss , gradient = get_current_parameters()\n",
    "velocity = past_velocity * momentum + learning_rate * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reverse_mode propagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1  = w0 + error(x(learning rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = mx + b\n",
    "y = mx + c\n",
    "\n",
    "basic derivation youtube tutorial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
